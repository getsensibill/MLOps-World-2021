{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torchvison as it is not installed.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall torchvison\n",
    "!pip install -qU torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Training using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is built from [SageMaker's PyTorch MNIST example](https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/pytorch_mnist). The objective here is to show how one might use SageMaker's Python SDK to build models and iterate.\n",
    "\n",
    "## Setup\n",
    "\n",
    "These next cells are copied from the example notebook. If you want a more thorough explanation of what each one is doing, read through the original example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15faf2a85c7440179252f7f8ca52b1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1886bb2e384040e0919dcf114e011ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a77b37b626449ca04efa338d2f979e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46288fe2e534c99b8412ffe9a1edd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "MNIST.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\"]\n",
    "\n",
    "MNIST(\n",
    "    'data',\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-ca-central-1-366756336356/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='mnist.py',\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=2,\n",
    "                    instance_type='ml.c5.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'backend': 'gloo'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-10 13:44:40 Starting - Starting the training job...\n",
      "2021-06-10 13:45:04 Starting - Launching requested ML instancesProfilerReport-1623332680: InProgress\n",
      "......\n",
      "2021-06-10 13:46:04 Starting - Preparing the instances for training............\n",
      "2021-06-10 13:48:08 Downloading - Downloading input data\n",
      "2021-06-10 13:48:08 Training - Downloading the training image..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:22,996 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:22,998 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:23,007 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:22,451 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:22,453 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:22,461 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:26,030 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2021-06-10 13:48:25 Training - Training image download completed. Training in progress.\u001b[35m2021-06-10 13:48:26,401 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:26,413 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:26,424 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:26,433 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2021-06-10-13-44-39-992\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-13-44-39-992/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"epochs\":1}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-13-44-39-992/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2021-06-10-13-44-39-992\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-13-44-39-992/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:28,685 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:29,054 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:29,065 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:29,076 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:29,085 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-06-10-13-44-39-992\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-13-44-39-992/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-13-44-39-992/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-06-10-13-44-39-992\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-13-44-39-992/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mGet train data loader\u001b[0m\n",
      "\u001b[35mGet test data loader\u001b[0m\n",
      "\u001b[35mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.201 algo-2:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.181 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.506 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.506 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.507 algo-1:26 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.507 algo-1:26 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.507 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.546 algo-1:26 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[2021-06-10 13:48:31.547 algo-1:26 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.563 algo-2:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.563 algo-2:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.564 algo-2:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.564 algo-2:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.564 algo-2:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.613 algo-2:25 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[35m[2021-06-10 13:48:31.614 algo-2:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.075230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.076306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.216741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.056925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.942084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.841636\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.911026\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.671317\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Processes 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)] Loss: 2.076306\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)] Loss: 1.056925\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)] Loss: 0.942084\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)] Loss: 0.841636\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-10 13:48:40,534 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[35mSaving the model.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Processes 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)] Loss: 2.075230\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)] Loss: 1.216741\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)] Loss: 0.911026\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)] Loss: 0.671317\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Saving the model.\n",
      "\u001b[0m\n",
      "\u001b[35m2021-06-10 13:48:40,560 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-06-10 13:49:05 Uploading - Uploading generated training model\n",
      "2021-06-10 13:49:05 Completed - Training job completed\n",
      "Training seconds: 112\n",
      "Billable seconds: 112\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our endpoint name in a variable so we can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Your Model\n",
    "\n",
    "Let's say you deployed your model and it's doing very well. You've set up some monitoring and several internal services are calling your endpoint. Let's try to improve the model by training it for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-10 14:46:04 Starting - Starting the training job...\n",
      "2021-06-10 14:46:28 Starting - Launching requested ML instancesProfilerReport-1623336364: InProgress\n",
      "......\n",
      "2021-06-10 14:47:28 Starting - Preparing the instances for training......\n",
      "2021-06-10 14:48:28 Downloading - Downloading input data...\n",
      "2021-06-10 14:48:58 Training - Downloading the training image..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:13,043 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:13,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:13,053 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:14,307 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:14,309 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:14,317 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:17,344 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:17,725 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:17,736 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:17,747 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-10 14:49:17,756 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-06-10-14-46-04-193\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-14-46-04-193/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-14-46-04-193/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-06-10-14-46-04-193\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-14-46-04-193/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:19,285 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:19,564 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:19,575 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:19,586 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-10 14:49:19,595 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2021-06-10-14-46-04-193\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-14-46-04-193/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"epochs\":10}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-14-46-04-193/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2021-06-10-14-46-04-193\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ca-central-1-366756336356/pytorch-training-2021-06-10-14-46-04-193/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"10\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mGet train data loader\u001b[0m\n",
      "\u001b[35mGet test data loader\u001b[0m\n",
      "\u001b[35mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:20.864 algo-2:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.144 algo-2:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.144 algo-2:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.144 algo-2:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.145 algo-2:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.145 algo-2:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.171 algo-2:25 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.171 algo-2:25 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[35m[2021-06-10 14:49:21.172 algo-2:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.075230\u001b[0m\n",
      "\n",
      "2021-06-10 14:49:28 Training - Training image download completed. Training in progress.\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:20.867 algo-1:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.164 algo-1:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.164 algo-1:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.164 algo-1:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.165 algo-1:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.165 algo-1:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.193 algo-1:25 INFO hook.py:584] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.193 algo-1:25 INFO hook.py:584] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.193 algo-1:25 INFO hook.py:584] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:584] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:584] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:584] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:584] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:584] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:586] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[2021-06-10 14:49:21.194 algo-1:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.076306\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.216741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.056925\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.911026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.942084\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.671317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.841636\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/30000 (21%)] Loss: 0.460633\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/30000 (43%)] Loss: 0.408843\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/30000 (64%)] Loss: 0.525474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/30000 (21%)] Loss: 0.508907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/30000 (43%)] Loss: 0.530827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/30000 (64%)] Loss: 0.454979\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/30000 (85%)] Loss: 0.558311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/30000 (85%)] Loss: 0.452728\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1941, Accuracy: 9433/10000 (94%)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1941, Accuracy: 9433/10000 (94%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/30000 (21%)] Loss: 0.303514\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [6400/30000 (21%)] Loss: 0.286623\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/30000 (43%)] Loss: 0.390065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/30000 (43%)] Loss: 0.416901\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/30000 (64%)] Loss: 0.350834\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/30000 (64%)] Loss: 0.387783\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/30000 (85%)] Loss: 0.486698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/30000 (85%)] Loss: 0.401608\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1499, Accuracy: 9548/10000 (95%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1499, Accuracy: 9548/10000 (95%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [6400/30000 (21%)] Loss: 0.340826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/30000 (21%)] Loss: 0.317133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/30000 (43%)] Loss: 0.414612\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/30000 (43%)] Loss: 0.215290\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19200/30000 (64%)] Loss: 0.337255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/30000 (64%)] Loss: 0.372578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/30000 (85%)] Loss: 0.339558\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/30000 (85%)] Loss: 0.365074\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1218, Accuracy: 9616/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1218, Accuracy: 9616/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/30000 (21%)] Loss: 0.286480\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [6400/30000 (21%)] Loss: 0.142678\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/30000 (43%)] Loss: 0.207063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/30000 (43%)] Loss: 0.267894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/30000 (64%)] Loss: 0.411006\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19200/30000 (64%)] Loss: 0.400892\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/30000 (85%)] Loss: 0.306497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/30000 (85%)] Loss: 0.348376\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1097, Accuracy: 9647/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1097, Accuracy: 9647/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [6400/30000 (21%)] Loss: 0.190191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/30000 (21%)] Loss: 0.371656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/30000 (43%)] Loss: 0.320532\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12800/30000 (43%)] Loss: 0.297894\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [19200/30000 (64%)] Loss: 0.249626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/30000 (64%)] Loss: 0.186840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/30000 (85%)] Loss: 0.256968\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [25600/30000 (85%)] Loss: 0.258117\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0964, Accuracy: 9692/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [6400/30000 (21%)] Loss: 0.206195\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0964, Accuracy: 9692/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/30000 (21%)] Loss: 0.336090\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [12800/30000 (43%)] Loss: 0.243996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/30000 (43%)] Loss: 0.250108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/30000 (64%)] Loss: 0.331017\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [19200/30000 (64%)] Loss: 0.264922\u001b[0m\n",
      "\u001b[35mTrain Epoch: 7 [25600/30000 (85%)] Loss: 0.304631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/30000 (85%)] Loss: 0.183427\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0884, Accuracy: 9719/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0884, Accuracy: 9719/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [6400/30000 (21%)] Loss: 0.191485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/30000 (21%)] Loss: 0.322429\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [12800/30000 (43%)] Loss: 0.200396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/30000 (43%)] Loss: 0.186960\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [19200/30000 (64%)] Loss: 0.210267\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/30000 (64%)] Loss: 0.245400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/30000 (85%)] Loss: 0.197328\u001b[0m\n",
      "\u001b[35mTrain Epoch: 8 [25600/30000 (85%)] Loss: 0.228986\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0836, Accuracy: 9744/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0836, Accuracy: 9744/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [6400/30000 (21%)] Loss: 0.214200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/30000 (21%)] Loss: 0.266800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/30000 (43%)] Loss: 0.252365\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [12800/30000 (43%)] Loss: 0.204134\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [19200/30000 (64%)] Loss: 0.122083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/30000 (64%)] Loss: 0.311024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/30000 (85%)] Loss: 0.173800\u001b[0m\n",
      "\u001b[35mTrain Epoch: 9 [25600/30000 (85%)] Loss: 0.223413\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0766, Accuracy: 9747/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0766, Accuracy: 9747/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/30000 (21%)] Loss: 0.253185\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [6400/30000 (21%)] Loss: 0.266187\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [12800/30000 (43%)] Loss: 0.246601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/30000 (43%)] Loss: 0.231688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/30000 (64%)] Loss: 0.367410\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [19200/30000 (64%)] Loss: 0.193720\u001b[0m\n",
      "\u001b[35mTrain Epoch: 10 [25600/30000 (85%)] Loss: 0.185191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/30000 (85%)] Loss: 0.282888\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.0735, Accuracy: 9773/10000 (98%)\n",
      "\u001b[0m\n",
      "\u001b[35mSaving the model.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Processes 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)] Loss: 2.075230\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)] Loss: 1.216741\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)] Loss: 0.911026\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)] Loss: 0.671317\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [6400/30000 (21%)] Loss: 0.460633\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [12800/30000 (43%)] Loss: 0.408843\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [19200/30000 (64%)] Loss: 0.525474\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 2 [25600/30000 (85%)] Loss: 0.558311\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1941, Accuracy: 9433/10000 (94%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [6400/30000 (21%)] Loss: 0.286623\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [12800/30000 (43%)] Loss: 0.390065\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [19200/30000 (64%)] Loss: 0.387783\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 3 [25600/30000 (85%)] Loss: 0.486698\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1499, Accuracy: 9548/10000 (95%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [6400/30000 (21%)] Loss: 0.340826\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [12800/30000 (43%)] Loss: 0.215290\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [19200/30000 (64%)] Loss: 0.337255\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 4 [25600/30000 (85%)] Loss: 0.365074\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1218, Accuracy: 9616/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [6400/30000 (21%)] Loss: 0.142678\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [12800/30000 (43%)] Loss: 0.207063\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [19200/30000 (64%)] Loss: 0.400892\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 5 [25600/30000 (85%)] Loss: 0.306497\u001b[0m\n",
      "\u001b[35m2021-06-10 14:50:45,627 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.1097, Accuracy: 9647/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [6400/30000 (21%)] Loss: 0.190191\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [12800/30000 (43%)] Loss: 0.297894\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [19200/30000 (64%)] Loss: 0.249626\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 6 [25600/30000 (85%)] Loss: 0.258117\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.0964, Accuracy: 9692/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 7 [6400/30000 (21%)] Loss: 0.206195\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 7 [12800/30000 (43%)] Loss: 0.243996\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 7 [19200/30000 (64%)] Loss: 0.264922\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 7 [25600/30000 (85%)] Loss: 0.304631\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.0884, Accuracy: 9719/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 8 [6400/30000 (21%)] Loss: 0.191485\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 8 [12800/30000 (43%)] Loss: 0.200396\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 8 [19200/30000 (64%)] Loss: 0.210267\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 8 [25600/30000 (85%)] Loss: 0.228986\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.0836, Accuracy: 9744/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 9 [6400/30000 (21%)] Loss: 0.214200\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 9 [12800/30000 (43%)] Loss: 0.204134\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 9 [19200/30000 (64%)] Loss: 0.122083\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 9 [25600/30000 (85%)] Loss: 0.223413\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.0766, Accuracy: 9747/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 10 [6400/30000 (21%)] Loss: 0.266187\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 10 [12800/30000 (43%)] Loss: 0.246601\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 10 [19200/30000 (64%)] Loss: 0.193720\u001b[0m\n",
      "\u001b[35mINFO:__main__:Train Epoch: 10 [25600/30000 (85%)] Loss: 0.185191\u001b[0m\n",
      "\u001b[35mINFO:__main__:Test set: Average loss: 0.0735, Accuracy: 9773/10000 (98%)\n",
      "\u001b[0m\n",
      "\u001b[35mINFO:__main__:Saving the model.\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0735, Accuracy: 9773/10000 (98%)\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Processes 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)] Loss: 2.076306\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)] Loss: 1.056925\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)] Loss: 0.942084\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)] Loss: 0.841636\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.3237, Accuracy: 9094/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/30000 (21%)] Loss: 0.508907\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/30000 (43%)] Loss: 0.530827\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/30000 (64%)] Loss: 0.454979\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/30000 (85%)] Loss: 0.452728\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1941, Accuracy: 9433/10000 (94%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/30000 (21%)] Loss: 0.303514\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/30000 (43%)] Loss: 0.416901\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/30000 (64%)] Loss: 0.350834\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/30000 (85%)] Loss: 0.401608\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1499, Accuracy: 9548/10000 (95%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/30000 (21%)] Loss: 0.317133\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/30000 (43%)] Loss: 0.414612\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/30000 (64%)] Loss: 0.372578\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/30000 (85%)] Loss: 0.339558\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1218, Accuracy: 9616/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/30000 (21%)] Loss: 0.286480\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/30000 (43%)] Loss: 0.267894\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/30000 (64%)] Loss: 0.411006\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/30000 (85%)] Loss: 0.348376\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1097, Accuracy: 9647/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/30000 (21%)] Loss: 0.371656\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/30000 (43%)] Loss: 0.320532\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/30000 (64%)] Loss: 0.186840\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/30000 (85%)] Loss: 0.256968\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0964, Accuracy: 9692/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/30000 (21%)] Loss: 0.336090\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/30000 (43%)] Loss: 0.250108\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/30000 (64%)] Loss: 0.331017\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/30000 (85%)] Loss: 0.183427\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0884, Accuracy: 9719/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/30000 (21%)] Loss: 0.322429\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/30000 (43%)] Loss: 0.186960\u001b[0m\n",
      "\u001b[34m2021-06-10 14:50:45,635 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/30000 (64%)] Loss: 0.245400\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/30000 (85%)] Loss: 0.197328\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0836, Accuracy: 9744/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/30000 (21%)] Loss: 0.266800\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/30000 (43%)] Loss: 0.252365\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/30000 (64%)] Loss: 0.311024\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/30000 (85%)] Loss: 0.173800\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0766, Accuracy: 9747/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/30000 (21%)] Loss: 0.253185\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/30000 (43%)] Loss: 0.231688\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/30000 (64%)] Loss: 0.367410\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/30000 (85%)] Loss: 0.282888\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0735, Accuracy: 9773/10000 (98%)\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-10 14:50:58 Uploading - Uploading generated training model\n",
      "2021-06-10 14:50:58 Completed - Training job completed\n",
      "Training seconds: 316\n",
      "Billable seconds: 316\n"
     ]
    }
   ],
   "source": [
    "second_estimator = PyTorch(entry_point='mnist.py',\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=2,\n",
    "                    instance_type='ml.c5.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,  # 10 epochs instead of 1\n",
    "                        'backend': 'gloo'\n",
    "                    })\n",
    "\n",
    "second_estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved our model. Great!\n",
    "\n",
    "### Deploying the Improved Model\n",
    "\n",
    "We want a way of deploying that model to the same endpoint we have set up before. It's normally a good idea to that instead of creating a new endpoint every time you train a new model, so we don't have to update our calling code.\n",
    "\n",
    "The [Using Estimators](https://sagemaker.readthedocs.io/en/stable/overview.html#using-estimators) section of the Python SDK documentation states that:\n",
    "\n",
    "> Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing SageMaker endpoint. This can be done by specifying the existing endpoint name for the `endpoint_name` parameter along with the `update_endpoint` parameter as True within your `deploy()` call.\n",
    "\n",
    "Then it goes ahead and shows us a code example doing just that:\n",
    "\n",
    "```python\n",
    "mxnet_predictor = mxnet_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type='ml.p2.xlarge',\n",
    "                                         update_endpoint=True,\n",
    "                                         endpoint_name='existing-endpoint')\n",
    "```\n",
    "\n",
    "So... Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'update_endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6c32138e7079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_kms_key\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_kms_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/pytorch/estimator.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, model_server_workers, role, vpc_config_override, entry_point, source_dir, dependencies, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mvpc_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vpc_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvpc_config_override\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependencies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         )\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/pytorch/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_data, role, entry_point, framework_version, py_version, image_uri, predictor_cls, model_server_workers, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         super(PyTorchModel, self).__init__(\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictor_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_data, image_uri, role, entry_point, source_dir, predictor_cls, env, name, container_log_level, code_location, sagemaker_session, dependencies, git_config, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         )\n\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'update_endpoint'"
     ]
    }
   ],
   "source": [
    "second_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    update_endpoint=True,\n",
    "    endpoint_name=ENDPOINT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird... This error message says that we're using deprecated stuff. Surpring, since we read in the `stable` version of the documentation that this should work. Let's read the link the error message gave us to see how we can fix this.\n",
    "\n",
    "> The `update_endpoint` argument in `deploy()` methods for estimators and models is now a no-op. Please use `sagemaker.predictor.Predictor.update_endpoint()` instead.\n",
    "\n",
    "Ok, so now we gotta have a `Predictor` before we deploy our model to the existing endpoint. [This piece of documentation](https://sagemaker.readthedocs.io/en/stable/overview.html#how-do-i-make-predictions-against-an-existing-endpoint) (in the same page that proved to be out of date) tells us that we can instantiate a new predictor by passing the existing endpoint name. Let's try that then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "existing_predictor = Predictor(ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can look at the [`Predictor.update_endpoint` method documentation](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.update_endpoint) to figure out how to update our existing endpoint.\n",
    "\n",
    "Give yourself a minute to try to figure this out...\n",
    "\n",
    "So what did you get? To me it seems that the `model_name` parameter holds promise, right? We're updating the endpoint to point to a new model. But what's our trained model name? Maybe our `second_estimator` instance holds the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_compiled_models',\n",
       " '_model_entry_point',\n",
       " '_model_source_dir',\n",
       " 'compile_model',\n",
       " 'create_model',\n",
       " 'model_channel_name',\n",
       " 'model_data',\n",
       " 'model_uri']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(second_estimator) if 'model' in m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok... Maybe `model_uri`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(second_estimator.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not it... Our not-so-much trustworthy [Python SDK](https://sagemaker.readthedocs.io/en/stable/) documentation doesn't seem to have an answer for us. \n",
    "We could spend some time on this, but let's just skip to the fix.\n",
    "\n",
    "If you're new to SageMaker you might be surprised to discover that **you haven't created a SageMaker model yet**. We have trained a model and we have the model saved on S3, but no \"Model object\" has been created. Normally the `Estimator.deploy()` method creates a model for you (although the [method documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.deploy) doesn't say anything about that). Since we're not using the `deploy()` method anymore, we gotta create the model manually using the `create_model` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = second_estimator.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model name: pytorch-training-2021-06-10-17-08-22-677 \n"
     ]
    }
   ],
   "source": [
    "print(f' Model name: {second_model.name} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Let's finally try the `Predictor.update_endpoint` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "existing_predictor.update_endpoint(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    model_name=second_model.name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hurrah! \n",
    "\n",
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"arn:aws:sagemaker:ca-central-1:366756336356:endpoint-config/pytorch-training-2021-06-10-13-49-23-10-2021-06-10-17-13-11-700\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-78e7bd7a06ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexisting_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexisting_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mdelete_endpoint\u001b[0;34m(self, delete_endpoint_config)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_delete_endpoint_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m\"\"\"Delete the Amazon SageMaker endpoint configuration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mcurrent_endpoint_config_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_endpoint_config_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_endpoint_config_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mdelete_endpoint_config\u001b[0;34m(self, endpoint_config_name)\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \"\"\"\n\u001b[1;32m   3033\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deleting endpoint configuration with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_config_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3034\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_config_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"arn:aws:sagemaker:ca-central-1:366756336356:endpoint-config/pytorch-training-2021-06-10-13-49-23-10-2021-06-10-17-13-11-700\"."
     ]
    }
   ],
   "source": [
    "existing_predictor.delete_endpoint()\n",
    "existing_predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## So what did we learn?\n",
    "\n",
    "Amazon should get their documentation straight. Every time you search for ways to train or deploy models the Python SDK seems to be the recommended way of doing that, but as we dig further and try to use it for our production pipelines we discover that the SDK is actually hiding a lot of stuff behind scenes that shouldn't be hidden! The documentation helps if you're following along the happy path of \"training a dummy model -> deploying to an endpoint\", but as soon as you try to customize this a bit you're bound to bump into troubles. \n",
    "\n",
    "Overall our recommendation is to **steer away from the Python SDK**. Instead, learn how to use Amazon's API/CLI to do all of your work. You'll get a much better grip of what is going on and how to customize things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
